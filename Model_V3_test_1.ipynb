{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "class WavFileHelper():\n",
    "    \n",
    "    def read_file_properties(self, filename):\n",
    "\n",
    "        wave_file = open(filename,\"rb\")\n",
    "        \n",
    "        riff = wave_file.read(12)\n",
    "        fmt = wave_file.read(36)\n",
    "        \n",
    "        num_channels_string = fmt[10:12]\n",
    "        num_channels = struct.unpack('<H', num_channels_string)[0]\n",
    "\n",
    "        sample_rate_string = fmt[12:16]\n",
    "        sample_rate = struct.unpack(\"<I\",sample_rate_string)[0]\n",
    "        \n",
    "        bit_depth_string = fmt[22:24]\n",
    "        bit_depth = struct.unpack(\"<H\",bit_depth_string)[0]\n",
    "\n",
    "        return (num_channels, sample_rate, bit_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavfilehelper = WavFileHelper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldatasetpath = 'E:\\\\2021\\\\dataset\\\\audio'\n",
    "\n",
    "metadata = pd.read_csv('E:\\\\2021\\\\dataset\\\\metadata\\\\metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "audiodata = []\n",
    "for index, row in metadata.iterrows():\n",
    "    \n",
    "    file_name = os.path.join(os.path.abspath('E:\\\\2021\\\\dataset\\\\audio'),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
    "    data = wavfilehelper.read_file_properties(file_name)\n",
    "    audiodata.append(data)\n",
    "\n",
    "#Convert into a Panda dataframe\n",
    "audiodf = pd.DataFrame(audiodata, columns=['num_channels','sample_rate','bit_depth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    0.868690\n",
      "1    0.131027\n",
      "0    0.000284\n",
      "Name: num_channels, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(audiodf.num_channels.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_channels</th>\n",
       "      <th>sample_rate</th>\n",
       "      <th>bit_depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>48000</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>48000</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>48000</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>48000</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>48000</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>48000</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>48000</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3496</th>\n",
       "      <td>1</td>\n",
       "      <td>11025</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3497</th>\n",
       "      <td>1</td>\n",
       "      <td>22050</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3498</th>\n",
       "      <td>1</td>\n",
       "      <td>22050</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3499</th>\n",
       "      <td>1</td>\n",
       "      <td>22050</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3500</th>\n",
       "      <td>1</td>\n",
       "      <td>22050</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3501</th>\n",
       "      <td>2</td>\n",
       "      <td>48000</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3502</th>\n",
       "      <td>2</td>\n",
       "      <td>48000</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3503</th>\n",
       "      <td>1</td>\n",
       "      <td>11025</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3504</th>\n",
       "      <td>1</td>\n",
       "      <td>8000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3505</th>\n",
       "      <td>1</td>\n",
       "      <td>22050</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3506</th>\n",
       "      <td>1</td>\n",
       "      <td>22050</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3507</th>\n",
       "      <td>1</td>\n",
       "      <td>11025</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3508</th>\n",
       "      <td>1</td>\n",
       "      <td>11025</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3509</th>\n",
       "      <td>2</td>\n",
       "      <td>11025</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3510</th>\n",
       "      <td>1</td>\n",
       "      <td>22050</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3511</th>\n",
       "      <td>1</td>\n",
       "      <td>32000</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3512</th>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3513</th>\n",
       "      <td>1</td>\n",
       "      <td>22050</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3514</th>\n",
       "      <td>1</td>\n",
       "      <td>11025</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3515</th>\n",
       "      <td>1</td>\n",
       "      <td>11025</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3516</th>\n",
       "      <td>1</td>\n",
       "      <td>11025</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3517</th>\n",
       "      <td>1</td>\n",
       "      <td>22050</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3518</th>\n",
       "      <td>1</td>\n",
       "      <td>22050</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3519</th>\n",
       "      <td>1</td>\n",
       "      <td>11025</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3520</th>\n",
       "      <td>1</td>\n",
       "      <td>11025</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3521</th>\n",
       "      <td>2</td>\n",
       "      <td>48000</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3522</th>\n",
       "      <td>2</td>\n",
       "      <td>48000</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3523</th>\n",
       "      <td>2</td>\n",
       "      <td>48000</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3524</th>\n",
       "      <td>1</td>\n",
       "      <td>11025</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3525</th>\n",
       "      <td>1</td>\n",
       "      <td>22050</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3526 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      num_channels  sample_rate  bit_depth\n",
       "0                2        44100         16\n",
       "1                2        44100         16\n",
       "2                2        44100         16\n",
       "3                2        44100         16\n",
       "4                2        44100         16\n",
       "5                2        44100         16\n",
       "6                2        44100         16\n",
       "7                2        44100         16\n",
       "8                2        44100         16\n",
       "9                2        44100         16\n",
       "10               2        44100         16\n",
       "11               2        44100         16\n",
       "12               1        48000         16\n",
       "13               1        48000         16\n",
       "14               1        48000         16\n",
       "15               2        44100         16\n",
       "16               2        44100         16\n",
       "17               2        44100         16\n",
       "18               2        44100         16\n",
       "19               2        44100         16\n",
       "20               2        44100         16\n",
       "21               2        44100         16\n",
       "22               1        48000         16\n",
       "23               1        48000         16\n",
       "24               1        48000         16\n",
       "25               2        44100         16\n",
       "26               2        44100         16\n",
       "27               2        44100         16\n",
       "28               2        44100         16\n",
       "29               2        48000         24\n",
       "...            ...          ...        ...\n",
       "3496             1        11025          8\n",
       "3497             1        22050         16\n",
       "3498             1        22050          8\n",
       "3499             1        22050          8\n",
       "3500             1        22050          8\n",
       "3501             2        48000         16\n",
       "3502             2        48000         16\n",
       "3503             1        11025          4\n",
       "3504             1         8000          8\n",
       "3505             1        22050          8\n",
       "3506             1        22050          8\n",
       "3507             1        11025         16\n",
       "3508             1        11025         16\n",
       "3509             2        11025          8\n",
       "3510             1        22050         16\n",
       "3511             1        32000         16\n",
       "3512             2        44100         16\n",
       "3513             1        22050         16\n",
       "3514             1        11025          8\n",
       "3515             1        11025         16\n",
       "3516             1        11025         16\n",
       "3517             1        22050         16\n",
       "3518             1        22050          8\n",
       "3519             1        11025          8\n",
       "3520             1        11025         16\n",
       "3521             2        48000         16\n",
       "3522             2        48000         16\n",
       "3523             2        48000         16\n",
       "3524             1        11025          8\n",
       "3525             1        22050         16\n",
       "\n",
       "[3526 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audiodf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.QuadMesh at 0x2b0ddf5ce48>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#mfcc for glass_breaking .wav file\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "\n",
    "file_name='E:\\\\2021\\\\dataset\\\\audio\\\\fold11\\\\7glass2.wav'\n",
    "\n",
    "audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "plt.figure(figsize=(15, 7))\n",
    "librosa.display.specshow(mfccs, sr=sample_rate, x_axis='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.QuadMesh at 0x1f576506cf8>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#mfcc for gunshot .wav file\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "\n",
    "file_name='E:\\\\2021\\\\UrbanSound8K\\\\audio\\\\fold10\\\\25037-6-0-0.wav'\n",
    "\n",
    "audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "plt.figure(figsize=(15, 7))\n",
    "librosa.display.specshow(mfccs, sr=sample_rate, x_axis='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_name):\n",
    "   \n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        mfccsscaled = np.mean(mfccs.T,axis=0)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file:\",file_name)\n",
    "        return None \n",
    "     \n",
    "    return mfccsscaled\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished feature extraction from  3518  files\n"
     ]
    }
   ],
   "source": [
    "# Load various imports \n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# Set the path to the full UrbanSound dataset \n",
    "fulldatasetpath = 'E:\\\\2021\\\\dataset\\\\audio'\n",
    "\n",
    "metadata = pd.read_csv('E:\\\\2021\\\\dataset\\\\metadata\\\\metadata.csv')\n",
    "\n",
    "features = []\n",
    "\n",
    "# Iterate through each sound file and extract the features \n",
    "for index, row in metadata.iterrows():\n",
    "    \n",
    "    file_name = os.path.join(os.path.abspath(fulldatasetpath),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
    "    \n",
    "    class_label = row[\"class\"]\n",
    "    data = extract_features(file_name)\n",
    "    \n",
    "    features.append([data, class_label])\n",
    "\n",
    "# Convert into a Panda dataframe \n",
    "featuresdf = pd.DataFrame(features, columns=['feature','class_label'])\n",
    "\n",
    "print('Finished feature extraction from ', len(featuresdf), ' files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "X = np.array(featuresdf.feature.tolist())\n",
    "y = np.array(featuresdf.class_label.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encode the classification labels\n",
    "le = LabelEncoder()\n",
    "yy = to_categorical(le.fit_transform(y)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((704, 40), (2814, 40))"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape,x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#from tensorflow.keras.utils import np_utils\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics \n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(256, input_shape=(40,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 256)               10496     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 5)                 1285      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 77,573\n",
      "Trainable params: 77,573\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "22/22 [==============================] - ETA: 2:04 - loss: 17.6259 - accuracy: 0.156 - ETA: 0s - loss: 15.5850 - accuracy: 0.1581  - 6s 5ms/step - loss: 15.4587 - accuracy: 0.1623\n",
      "Pre-training accuracy: 17.7557%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "# Display model architecture summary \n",
    "model.summary()\n",
    "\n",
    "# Calculate pre-training accuracy \n",
    "score = model.evaluate(x_test, y_test)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "print(\"Pre-training accuracy: %.4f%%\" % accuracy) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2820,), (2820, 5), (706,), (706, 5))"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,y_train.shape,x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/72\n",
      "11/11 [==============================] - ETA: 12s - loss: 34.3688 - accuracy: 0.19 - ETA: 0s - loss: 25.1478 - accuracy: 0.2523 - ETA: 0s - loss: 21.3553 - accuracy: 0.288 - 2s 96ms/step - loss: 20.0266 - accuracy: 0.3021 - val_loss: 5.1448 - val_accuracy: 0.5653\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.14476, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 2/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 12.5114 - accuracy: 0.406 - ETA: 0s - loss: 10.7615 - accuracy: 0.451 - ETA: 0s - loss: 9.5443 - accuracy: 0.474 - 0s 21ms/step - loss: 9.4131 - accuracy: 0.4783 - val_loss: 2.1683 - val_accuracy: 0.6364\n",
      "\n",
      "Epoch 00002: val_loss improved from 5.14476 to 2.16833, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 3/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 8.9295 - accuracy: 0.44 - ETA: 0s - loss: 7.2510 - accuracy: 0.46 - ETA: 0s - loss: 6.5370 - accuracy: 0.46 - 0s 20ms/step - loss: 6.3668 - accuracy: 0.4758 - val_loss: 1.5178 - val_accuracy: 0.6562\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.16833 to 1.51782, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 4/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 4.5726 - accuracy: 0.51 - ETA: 0s - loss: 4.2810 - accuracy: 0.54 - ETA: 0s - loss: 4.0141 - accuracy: 0.55 - 0s 19ms/step - loss: 3.9619 - accuracy: 0.5476 - val_loss: 1.1013 - val_accuracy: 0.6406\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.51782 to 1.10125, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 5/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 3.5956 - accuracy: 0.55 - ETA: 0s - loss: 3.3960 - accuracy: 0.53 - 0s 16ms/step - loss: 3.1016 - accuracy: 0.5423 - val_loss: 0.8629 - val_accuracy: 0.6818\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.10125 to 0.86294, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 6/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 2.5245 - accuracy: 0.61 - ETA: 0s - loss: 2.2955 - accuracy: 0.57 - ETA: 0s - loss: 2.1864 - accuracy: 0.56 - 3s 284ms/step - loss: 2.1864 - accuracy: 0.5686 - val_loss: 0.9015 - val_accuracy: 0.6804\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.86294\n",
      "Epoch 7/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.6400 - accuracy: 0.59 - ETA: 0s - loss: 2.0412 - accuracy: 0.56 - ETA: 0s - loss: 1.9933 - accuracy: 0.57 - 0s 24ms/step - loss: 1.9852 - accuracy: 0.5700 - val_loss: 0.9636 - val_accuracy: 0.6804\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.86294\n",
      "Epoch 8/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.6982 - accuracy: 0.55 - ETA: 0s - loss: 1.7712 - accuracy: 0.56 - ETA: 0s - loss: 1.7224 - accuracy: 0.56 - 0s 25ms/step - loss: 1.6920 - accuracy: 0.5725 - val_loss: 1.0106 - val_accuracy: 0.6776\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.86294\n",
      "Epoch 9/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.8106 - accuracy: 0.53 - ETA: 0s - loss: 1.6285 - accuracy: 0.55 - ETA: 0s - loss: 1.5323 - accuracy: 0.57 - 0s 21ms/step - loss: 1.5124 - accuracy: 0.5693 - val_loss: 1.0196 - val_accuracy: 0.7045\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.86294\n",
      "Epoch 10/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.4240 - accuracy: 0.58 - ETA: 0s - loss: 1.3643 - accuracy: 0.61 - ETA: 0s - loss: 1.3577 - accuracy: 0.59 - 0s 26ms/step - loss: 1.3510 - accuracy: 0.5942 - val_loss: 1.0333 - val_accuracy: 0.7031\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.86294\n",
      "Epoch 11/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3062 - accuracy: 0.59 - ETA: 0s - loss: 1.3652 - accuracy: 0.58 - ETA: 0s - loss: 1.3077 - accuracy: 0.59 - 0s 27ms/step - loss: 1.2804 - accuracy: 0.5878 - val_loss: 1.0286 - val_accuracy: 0.7116\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.86294\n",
      "Epoch 12/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.9181 - accuracy: 0.64 - ETA: 0s - loss: 1.0957 - accuracy: 0.61 - ETA: 0s - loss: 1.1293 - accuracy: 0.61 - 0s 26ms/step - loss: 1.1439 - accuracy: 0.6073 - val_loss: 0.9974 - val_accuracy: 0.7088\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.86294\n",
      "Epoch 13/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.0952 - accuracy: 0.58 - ETA: 0s - loss: 1.0934 - accuracy: 0.61 - ETA: 0s - loss: 1.0989 - accuracy: 0.60 - 0s 25ms/step - loss: 1.1038 - accuracy: 0.6109 - val_loss: 0.9575 - val_accuracy: 0.7131\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.86294\n",
      "Epoch 14/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.0929 - accuracy: 0.58 - ETA: 0s - loss: 1.0671 - accuracy: 0.61 - ETA: 0s - loss: 1.0542 - accuracy: 0.62 - 0s 26ms/step - loss: 1.0646 - accuracy: 0.6247 - val_loss: 0.9386 - val_accuracy: 0.7159\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.86294\n",
      "Epoch 15/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.0627 - accuracy: 0.63 - ETA: 0s - loss: 0.9810 - accuracy: 0.63 - ETA: 0s - loss: 1.0095 - accuracy: 0.62 - 0s 28ms/step - loss: 1.0127 - accuracy: 0.6318 - val_loss: 0.9146 - val_accuracy: 0.7202\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.86294\n",
      "Epoch 16/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.9193 - accuracy: 0.67 - ETA: 0s - loss: 1.0627 - accuracy: 0.63 - ETA: 0s - loss: 1.0417 - accuracy: 0.64 - 0s 22ms/step - loss: 1.0391 - accuracy: 0.6475 - val_loss: 0.8989 - val_accuracy: 0.7315\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.86294\n",
      "Epoch 17/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.0111 - accuracy: 0.59 - ETA: 0s - loss: 1.0027 - accuracy: 0.62 - 0s 14ms/step - loss: 0.9993 - accuracy: 0.6311 - val_loss: 0.8948 - val_accuracy: 0.7372\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.86294\n",
      "Epoch 18/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.8673 - accuracy: 0.70 - ETA: 0s - loss: 0.9531 - accuracy: 0.65 - 0s 14ms/step - loss: 0.9821 - accuracy: 0.6400 - val_loss: 0.8771 - val_accuracy: 0.7358\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.86294\n",
      "Epoch 19/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.0116 - accuracy: 0.64 - ETA: 0s - loss: 0.9510 - accuracy: 0.64 - 0s 14ms/step - loss: 0.9361 - accuracy: 0.6517 - val_loss: 0.8735 - val_accuracy: 0.7401\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.86294\n",
      "Epoch 20/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.0404 - accuracy: 0.63 - ETA: 0s - loss: 0.9305 - accuracy: 0.66 - 0s 22ms/step - loss: 0.9050 - accuracy: 0.6642 - val_loss: 0.8449 - val_accuracy: 0.7386\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.86294 to 0.84486, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 21/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.9910 - accuracy: 0.66 - ETA: 0s - loss: 0.8539 - accuracy: 0.68 - ETA: 0s - loss: 0.8909 - accuracy: 0.67 - 0s 25ms/step - loss: 0.8971 - accuracy: 0.6688 - val_loss: 0.8018 - val_accuracy: 0.7401\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.84486 to 0.80176, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 22/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.9432 - accuracy: 0.67 - ETA: 0s - loss: 0.9188 - accuracy: 0.65 - ETA: 0s - loss: 0.8686 - accuracy: 0.66 - 0s 21ms/step - loss: 0.8753 - accuracy: 0.6695 - val_loss: 0.8165 - val_accuracy: 0.7443\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.80176\n",
      "Epoch 23/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.8890 - accuracy: 0.66 - ETA: 0s - loss: 0.8829 - accuracy: 0.66 - ETA: 0s - loss: 0.8694 - accuracy: 0.66 - 0s 21ms/step - loss: 0.8661 - accuracy: 0.6652 - val_loss: 0.7879 - val_accuracy: 0.7401\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.80176 to 0.78786, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 24/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.7548 - accuracy: 0.69 - ETA: 0s - loss: 0.8513 - accuracy: 0.66 - ETA: 0s - loss: 0.8289 - accuracy: 0.67 - 0s 19ms/step - loss: 0.8476 - accuracy: 0.6759 - val_loss: 0.7807 - val_accuracy: 0.7472\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.78786 to 0.78072, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 25/72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 0.9072 - accuracy: 0.65 - ETA: 0s - loss: 0.8738 - accuracy: 0.66 - ETA: 0s - loss: 0.8656 - accuracy: 0.67 - 0s 20ms/step - loss: 0.8661 - accuracy: 0.6745 - val_loss: 0.7816 - val_accuracy: 0.7472\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.78072\n",
      "Epoch 26/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.8864 - accuracy: 0.67 - ETA: 0s - loss: 0.8112 - accuracy: 0.69 - 0s 25ms/step - loss: 0.8250 - accuracy: 0.6859 - val_loss: 0.7655 - val_accuracy: 0.7472\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.78072 to 0.76545, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 27/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.8624 - accuracy: 0.66 - ETA: 0s - loss: 0.7996 - accuracy: 0.69 - 0s 16ms/step - loss: 0.7911 - accuracy: 0.6965 - val_loss: 0.7467 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.76545 to 0.74667, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 28/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.7681 - accuracy: 0.71 - ETA: 0s - loss: 0.8135 - accuracy: 0.70 - 0s 17ms/step - loss: 0.7975 - accuracy: 0.7122 - val_loss: 0.7132 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.74667 to 0.71315, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 29/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.6807 - accuracy: 0.75 - ETA: 0s - loss: 0.7652 - accuracy: 0.71 - 0s 16ms/step - loss: 0.7694 - accuracy: 0.7086 - val_loss: 0.7102 - val_accuracy: 0.7642\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.71315 to 0.71019, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 30/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.7137 - accuracy: 0.72 - ETA: 0s - loss: 0.7528 - accuracy: 0.70 - 0s 16ms/step - loss: 0.7580 - accuracy: 0.7178 - val_loss: 0.6954 - val_accuracy: 0.7670\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.71019 to 0.69535, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 31/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.8030 - accuracy: 0.70 - ETA: 0s - loss: 0.7846 - accuracy: 0.69 - ETA: 0s - loss: 0.7802 - accuracy: 0.70 - 0s 19ms/step - loss: 0.7802 - accuracy: 0.7026 - val_loss: 0.6801 - val_accuracy: 0.7727\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.69535 to 0.68010, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 32/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.7101 - accuracy: 0.74 - ETA: 0s - loss: 0.7681 - accuracy: 0.70 - 0s 16ms/step - loss: 0.7477 - accuracy: 0.7125 - val_loss: 0.6847 - val_accuracy: 0.7741\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.68010\n",
      "Epoch 33/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.8064 - accuracy: 0.70 - ETA: 0s - loss: 0.7447 - accuracy: 0.71 - 0s 16ms/step - loss: 0.7467 - accuracy: 0.7146 - val_loss: 0.6742 - val_accuracy: 0.7713\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.68010 to 0.67415, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 34/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.7405 - accuracy: 0.75 - ETA: 0s - loss: 0.7156 - accuracy: 0.72 - ETA: 0s - loss: 0.7267 - accuracy: 0.71 - 0s 19ms/step - loss: 0.7267 - accuracy: 0.7186 - val_loss: 0.6529 - val_accuracy: 0.7727\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.67415 to 0.65295, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 35/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.7091 - accuracy: 0.75 - ETA: 0s - loss: 0.7052 - accuracy: 0.74 - 0s 16ms/step - loss: 0.7084 - accuracy: 0.7331 - val_loss: 0.6407 - val_accuracy: 0.7784\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.65295 to 0.64069, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 36/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.7284 - accuracy: 0.71 - ETA: 0s - loss: 0.6949 - accuracy: 0.72 - ETA: 0s - loss: 0.7080 - accuracy: 0.71 - 0s 18ms/step - loss: 0.7080 - accuracy: 0.7164 - val_loss: 0.6281 - val_accuracy: 0.7784\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.64069 to 0.62808, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 37/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.5781 - accuracy: 0.75 - ETA: 0s - loss: 0.7027 - accuracy: 0.72 - 0s 16ms/step - loss: 0.7080 - accuracy: 0.7228 - val_loss: 0.6331 - val_accuracy: 0.7784\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.62808\n",
      "Epoch 38/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.6065 - accuracy: 0.79 - ETA: 0s - loss: 0.6972 - accuracy: 0.72 - 0s 16ms/step - loss: 0.6850 - accuracy: 0.7331 - val_loss: 0.6214 - val_accuracy: 0.7770\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.62808 to 0.62144, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 39/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.7644 - accuracy: 0.74 - ETA: 0s - loss: 0.6962 - accuracy: 0.73 - ETA: 0s - loss: 0.6956 - accuracy: 0.72 - 0s 18ms/step - loss: 0.6956 - accuracy: 0.7271 - val_loss: 0.6097 - val_accuracy: 0.7827\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.62144 to 0.60975, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 40/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.7535 - accuracy: 0.67 - ETA: 0s - loss: 0.6712 - accuracy: 0.72 - 0s 16ms/step - loss: 0.6609 - accuracy: 0.7324 - val_loss: 0.5974 - val_accuracy: 0.7756\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.60975 to 0.59742, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 41/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.6538 - accuracy: 0.75 - ETA: 0s - loss: 0.6615 - accuracy: 0.74 - ETA: 0s - loss: 0.6682 - accuracy: 0.74 - 0s 20ms/step - loss: 0.6682 - accuracy: 0.7409 - val_loss: 0.6007 - val_accuracy: 0.7940\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.59742\n",
      "Epoch 42/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.5651 - accuracy: 0.79 - ETA: 0s - loss: 0.6451 - accuracy: 0.75 - 0s 16ms/step - loss: 0.6655 - accuracy: 0.7463 - val_loss: 0.5924 - val_accuracy: 0.7869\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.59742 to 0.59237, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 43/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.6760 - accuracy: 0.73 - ETA: 0s - loss: 0.6541 - accuracy: 0.75 - ETA: 0s - loss: 0.6612 - accuracy: 0.74 - 0s 18ms/step - loss: 0.6612 - accuracy: 0.7452 - val_loss: 0.5911 - val_accuracy: 0.7756\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.59237 to 0.59113, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 44/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.7553 - accuracy: 0.73 - ETA: 0s - loss: 0.6517 - accuracy: 0.75 - ETA: 0s - loss: 0.6452 - accuracy: 0.75 - 0s 19ms/step - loss: 0.6452 - accuracy: 0.7584 - val_loss: 0.5809 - val_accuracy: 0.7898\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.59113 to 0.58086, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 45/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.6476 - accuracy: 0.75 - ETA: 0s - loss: 0.6562 - accuracy: 0.75 - ETA: 0s - loss: 0.6418 - accuracy: 0.76 - 0s 18ms/step - loss: 0.6418 - accuracy: 0.7605 - val_loss: 0.5764 - val_accuracy: 0.7898\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.58086 to 0.57643, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 46/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.5386 - accuracy: 0.78 - ETA: 0s - loss: 0.6446 - accuracy: 0.75 - ETA: 0s - loss: 0.6375 - accuracy: 0.75 - 0s 24ms/step - loss: 0.6375 - accuracy: 0.7520 - val_loss: 0.5840 - val_accuracy: 0.7912\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.57643\n",
      "Epoch 47/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.7028 - accuracy: 0.70 - ETA: 0s - loss: 0.6224 - accuracy: 0.75 - ETA: 0s - loss: 0.6211 - accuracy: 0.74 - 0s 20ms/step - loss: 0.6211 - accuracy: 0.7477 - val_loss: 0.5630 - val_accuracy: 0.7969\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.57643 to 0.56304, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 48/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.6712 - accuracy: 0.74 - ETA: 0s - loss: 0.6342 - accuracy: 0.75 - ETA: 0s - loss: 0.6253 - accuracy: 0.75 - 0s 18ms/step - loss: 0.6253 - accuracy: 0.7523 - val_loss: 0.5498 - val_accuracy: 0.8011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00048: val_loss improved from 0.56304 to 0.54983, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 49/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.5901 - accuracy: 0.75 - ETA: 0s - loss: 0.6210 - accuracy: 0.76 - 0s 16ms/step - loss: 0.6223 - accuracy: 0.7612 - val_loss: 0.5331 - val_accuracy: 0.8068\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.54983 to 0.53307, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 50/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.5365 - accuracy: 0.75 - ETA: 0s - loss: 0.6039 - accuracy: 0.76 - 0s 16ms/step - loss: 0.5998 - accuracy: 0.7630 - val_loss: 0.5394 - val_accuracy: 0.8011\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.53307\n",
      "Epoch 51/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.6209 - accuracy: 0.75 - ETA: 0s - loss: 0.6099 - accuracy: 0.76 - 0s 16ms/step - loss: 0.6095 - accuracy: 0.7687 - val_loss: 0.5262 - val_accuracy: 0.8026\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.53307 to 0.52625, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 52/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.5716 - accuracy: 0.77 - ETA: 0s - loss: 0.5427 - accuracy: 0.78 - 0s 16ms/step - loss: 0.5816 - accuracy: 0.7765 - val_loss: 0.5191 - val_accuracy: 0.8026\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.52625 to 0.51909, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 53/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.5411 - accuracy: 0.78 - ETA: 0s - loss: 0.5934 - accuracy: 0.76 - 0s 16ms/step - loss: 0.5966 - accuracy: 0.7690 - val_loss: 0.5226 - val_accuracy: 0.8068\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.51909\n",
      "Epoch 54/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.4872 - accuracy: 0.80 - ETA: 0s - loss: 0.5816 - accuracy: 0.76 - 0s 16ms/step - loss: 0.5707 - accuracy: 0.7733 - val_loss: 0.5053 - val_accuracy: 0.8168\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.51909 to 0.50526, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 55/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.5432 - accuracy: 0.79 - ETA: 0s - loss: 0.5680 - accuracy: 0.77 - 0s 24ms/step - loss: 0.5627 - accuracy: 0.7800 - val_loss: 0.5005 - val_accuracy: 0.8168\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.50526 to 0.50054, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 56/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.6175 - accuracy: 0.76 - ETA: 0s - loss: 0.5731 - accuracy: 0.78 - ETA: 0s - loss: 0.5685 - accuracy: 0.78 - 0s 18ms/step - loss: 0.5685 - accuracy: 0.7843 - val_loss: 0.4992 - val_accuracy: 0.8239\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.50054 to 0.49924, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 57/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.4537 - accuracy: 0.82 - ETA: 0s - loss: 0.5589 - accuracy: 0.77 - 0s 17ms/step - loss: 0.5563 - accuracy: 0.7846 - val_loss: 0.5022 - val_accuracy: 0.8239\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.49924\n",
      "Epoch 58/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.6327 - accuracy: 0.80 - ETA: 0s - loss: 0.5589 - accuracy: 0.78 - 0s 16ms/step - loss: 0.5552 - accuracy: 0.7878 - val_loss: 0.4931 - val_accuracy: 0.8224\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.49924 to 0.49305, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 59/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.4067 - accuracy: 0.84 - ETA: 0s - loss: 0.5441 - accuracy: 0.78 - ETA: 0s - loss: 0.5532 - accuracy: 0.77 - 0s 19ms/step - loss: 0.5532 - accuracy: 0.7783 - val_loss: 0.4826 - val_accuracy: 0.8210\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.49305 to 0.48264, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 60/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.5261 - accuracy: 0.76 - ETA: 0s - loss: 0.5229 - accuracy: 0.79 - 0s 16ms/step - loss: 0.5431 - accuracy: 0.7875 - val_loss: 0.4909 - val_accuracy: 0.8295\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.48264\n",
      "Epoch 61/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.5225 - accuracy: 0.82 - ETA: 0s - loss: 0.5166 - accuracy: 0.80 - 0s 16ms/step - loss: 0.5300 - accuracy: 0.7910 - val_loss: 0.4699 - val_accuracy: 0.8239\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.48264 to 0.46986, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 62/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.4475 - accuracy: 0.81 - ETA: 0s - loss: 0.5076 - accuracy: 0.79 - ETA: 0s - loss: 0.5231 - accuracy: 0.78 - 0s 19ms/step - loss: 0.5231 - accuracy: 0.7878 - val_loss: 0.4738 - val_accuracy: 0.8366\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.46986\n",
      "Epoch 63/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.4956 - accuracy: 0.80 - ETA: 0s - loss: 0.5379 - accuracy: 0.79 - 0s 17ms/step - loss: 0.5273 - accuracy: 0.7953 - val_loss: 0.4622 - val_accuracy: 0.8395\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.46986 to 0.46224, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 64/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.4814 - accuracy: 0.83 - ETA: 0s - loss: 0.5465 - accuracy: 0.79 - ETA: 0s - loss: 0.5302 - accuracy: 0.79 - 0s 23ms/step - loss: 0.5480 - accuracy: 0.7893 - val_loss: 0.4599 - val_accuracy: 0.8324\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.46224 to 0.45993, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 65/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.4677 - accuracy: 0.83 - ETA: 0s - loss: 0.5051 - accuracy: 0.79 - ETA: 0s - loss: 0.5214 - accuracy: 0.79 - 0s 23ms/step - loss: 0.5214 - accuracy: 0.7900 - val_loss: 0.4641 - val_accuracy: 0.8338\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.45993\n",
      "Epoch 66/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.5349 - accuracy: 0.77 - ETA: 0s - loss: 0.5127 - accuracy: 0.78 - ETA: 0s - loss: 0.5108 - accuracy: 0.78 - 0s 22ms/step - loss: 0.5052 - accuracy: 0.7910 - val_loss: 0.4543 - val_accuracy: 0.8395\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.45993 to 0.45432, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 67/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.4742 - accuracy: 0.82 - ETA: 0s - loss: 0.5090 - accuracy: 0.81 - ETA: 0s - loss: 0.5166 - accuracy: 0.81 - 0s 24ms/step - loss: 0.5171 - accuracy: 0.8092 - val_loss: 0.4480 - val_accuracy: 0.8381\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.45432 to 0.44800, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 68/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.5397 - accuracy: 0.79 - ETA: 0s - loss: 0.5279 - accuracy: 0.79 - ETA: 0s - loss: 0.5036 - accuracy: 0.80 - 0s 22ms/step - loss: 0.4986 - accuracy: 0.8053 - val_loss: 0.4450 - val_accuracy: 0.8395\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.44800 to 0.44502, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 69/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.4194 - accuracy: 0.83 - ETA: 0s - loss: 0.4670 - accuracy: 0.81 - ETA: 0s - loss: 0.4800 - accuracy: 0.80 - 0s 22ms/step - loss: 0.4772 - accuracy: 0.8134 - val_loss: 0.4474 - val_accuracy: 0.8324\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.44502\n",
      "Epoch 70/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.4635 - accuracy: 0.82 - ETA: 0s - loss: 0.4894 - accuracy: 0.81 - ETA: 0s - loss: 0.4931 - accuracy: 0.80 - 0s 24ms/step - loss: 0.4933 - accuracy: 0.8049 - val_loss: 0.4319 - val_accuracy: 0.8381\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.44502 to 0.43188, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 71/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.5639 - accuracy: 0.80 - ETA: 0s - loss: 0.5015 - accuracy: 0.80 - ETA: 0s - loss: 0.4981 - accuracy: 0.80 - 0s 23ms/step - loss: 0.4995 - accuracy: 0.8042 - val_loss: 0.4318 - val_accuracy: 0.8395\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.43188 to 0.43181, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "Epoch 72/72\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.4662 - accuracy: 0.82 - ETA: 0s - loss: 0.4772 - accuracy: 0.81 - ETA: 0s - loss: 0.4838 - accuracy: 0.80 - 0s 27ms/step - loss: 0.4794 - accuracy: 0.8120 - val_loss: 0.4388 - val_accuracy: 0.8381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00072: val_loss did not improve from 0.43181\n",
      "Training completed in time:  0:00:32.616373\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint \n",
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 72\n",
    "num_batch_size = 256\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_cnn.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.8610518574714661\n",
      "Testing Accuracy:  0.8380681872367859\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prediction(file_name):\n",
    "    prediction_feature = extract_feature(file_name) \n",
    "\n",
    "    predicted_vector = model.predict_classes(prediction_feature)\n",
    "    predicted_class = le.inverse_transform(predicted_vector) \n",
    "    print(\"The predicted class is:\", predicted_class[0], '\\n') \n",
    "\n",
    "    predicted_proba_vector = model.predict_proba(prediction_feature) \n",
    "    predicted_proba = predicted_proba_vector[0]\n",
    "    for i in range(len(predicted_proba)): \n",
    "        category = le.inverse_transform(np.array([i]))\n",
    "        print(category[0], \"\\t\\t : \", format(predicted_proba[i], '.32f') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa \n",
    "import numpy as np \n",
    "\n",
    "def extract_feature(file_name):\n",
    "   \n",
    "    try:\n",
    "        audio_data, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "        mfccs = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=40)\n",
    "        mfccsscaled = np.mean(mfccs.T,axis=0)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None, None\n",
    "\n",
    "    return np.array([mfccsscaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: drilling \n",
      "\n",
      "dog_bark \t\t :  0.01914244703948497772216796875000\n",
      "drilling \t\t :  0.96431100368499755859375000000000\n",
      "glass_breaking \t\t :  0.00329884910024702548980712890625\n",
      "gun_shot \t\t :  0.01324514672160148620605468750000\n",
      "jackhammer \t\t :  0.00000268885059995227493345737457\n"
     ]
    }
   ],
   "source": [
    "filename = 'E:\\\\2021\\\\dataset\\\\audio\\\\fold2\\\\76086-4-0-36.wav'\n",
    "print_prediction(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: gun_shot \n",
      "\n",
      "dog_bark \t\t :  0.29386383295059204101562500000000\n",
      "drilling \t\t :  0.17272800207138061523437500000000\n",
      "glass_breaking \t\t :  0.21439269185066223144531250000000\n",
      "gun_shot \t\t :  0.31107616424560546875000000000000\n",
      "jackhammer \t\t :  0.00793931726366281509399414062500\n"
     ]
    }
   ],
   "source": [
    "filename = 'E:\\\\2021\\\\dataset\\\\audio\\\\fold2\\\\76090-6-1-0.wav'\n",
    "print_prediction(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
